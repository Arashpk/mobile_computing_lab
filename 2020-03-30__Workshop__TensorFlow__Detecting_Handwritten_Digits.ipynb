{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-03-30 Workshop: TensorFlow Detecting Handwritten Digits\n",
    "\n",
    "This is a handwritten character image (MNIST) classifier that can run on any Android device. The app stores a model  set of images (0-9) that we can cycle through and classify in order. It uses a pre-trained model to perform inference on the device. This idea can be applied to any images, both by using the camera and by pulling from the Web. We're using preloaded images so we can run the app in a simulator (no need for the device since it doesn't require a camera).\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=gahi0Hjgokw\"><b>Video Demo</b></a>\n",
    "* <a href=\"https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android/tree/master/mnistandroid\"><b>App Source Code</b></a>\n",
    "\n",
    "<img src=\"https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android/raw/master/images/demo.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## 1. Building / Training / Testing a Model\n",
    "\n",
    "### Data \n",
    "\n",
    "MNIST is a simple computer vision dataset (70'000 labeled examples). It consists of 28x28 pixel images of handwritten digits. Every MNIST data point, every image, can be thought of as an array of numbers describing how dark each pixel is. For example, we might think of 1 as something like:\n",
    "\n",
    "<img src=\"https://tensorflow.rstudio.com/tensorflow/articles/images/MNIST-Matrix.png\" width=\"600\">\n",
    "\n",
    "Since each image has 28 by 28 pixels, we get a 28x28 array. We can flatten each array into a 28âˆ—28=784\n",
    " dimensional vector. Each component of the vector is a value between zero and one describing the intensity of the pixel. Thus, we generally think of MNIST as being a collection of 784-dimensional vectors.\n",
    " \n",
    "The __y_train__ data is the associated labels for all the __x_train__ examples. Rather than storing the label as an integer, it is stored as a 1x10 binary array with the one representing the digit. This is also known as one-hot encoding. In the example below, the array represents a 7:\n",
    "\n",
    "<img src=\"https://d3ansictanv2wj.cloudfront.net/Img-1-array-b4889b9860c9e009bbd58e827a114129.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.7.7\n",
    "# tensorflow 2.1.0\n",
    "# Keras 2.3.1\n",
    "\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    y_train = tensorflow.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tensorflow.keras.utils.to_categorical(y_test, 10)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahinea/work/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_digit(digit, title = \"\"):\n",
    "    \"\"\"\n",
    "    graphically displays a 784x1 vector, representing a digit\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    fig = plt.imshow(digit.flatten().reshape(28,28))\n",
    "    fig.set_cmap('gray_r')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    if title != \"\":\n",
    "        plt.title(\"Inferred label: \" + str(title))\n",
    "    plt.show()\n",
    "        \n",
    "digit = 129\n",
    "display_digit(x_train[digit], title = y_train[digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model\n",
    "\n",
    "Read: <a href=\"https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59\">Visualizing parts of Convolutional Neural Networks using Keras and Cats</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu', \\\n",
    "            input_shape=[28, 28, 1]))\n",
    "    # 28*28*64\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    \n",
    "    # 14*14*64\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu'))\n",
    "\n",
    "    # 14*14*128\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    \n",
    "    # 7*7*128\n",
    "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu'))\n",
    "    \n",
    "    # 7*7*256\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    \n",
    "    # 4*4*256\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 4,575,242\n",
      "Trainable params: 4,575,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Testing / Validation\n",
    "\n",
    "When solving with a CPU an Optimization Problem, you Iteratively apply an Algorithm over some Input Data. In each of these iterations you usually update a Metric of your problem doing some Calculations on the Data. Now when the size of your data is large it might need a considerable amount of time to complete every iteration, and may consume a lot of resources. So sometimes you choose to apply these iterative calculations on a Portion of the Data to save time and computational resources. This portion is the batch_size and the process is called (in the Neural Network Lingo) batch data processing.\n",
    "\n",
    "In the neural network terminology:\n",
    "* one __epoch__ = one forward pass and one backward pass of all the training examples\n",
    "* __batch size__ = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
    "* number of __iterations__ = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
    "\n",
    "Advantages:\n",
    "* It requires less memory. Since you train network using less number of samples the overall training procedure requires less memory. It's especially important in case if you are not able to fit dataset in memory.\n",
    "* Typically networks trains faster with mini-batches. That's because we update weights after each propagation. In our example we've propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) and after each of them we've updated network's parameters. If we used all samples during propagation we would make only 1 update for the network's parameter.\n",
    "\n",
    "Disadvantages:\n",
    "* The smaller the batch the less accurate estimate of the gradient. In the figure below you can see that mini-batch (green color) gradient's direction fluctuates compare to the full batch (blue color).\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/lU3sx.png\" width=\"700\">\n",
    "\n",
    "Training code below takes around 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 2.2902 - accuracy: 0.1266\n",
      "Epoch 00001: saving model to ws3_hwd_data/cp.ckpt\n",
      "60000/60000 [==============================] - 75s 1ms/sample - loss: 2.2902 - accuracy: 0.1268 - val_loss: 2.2745 - val_accuracy: 0.2139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x161040690>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, \\\n",
    "    optimizer=tensorflow.keras.optimizers.Adadelta(), \\\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "checkpoint_path = \"ws3_hwd_data/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tensorflow.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "model.fit(x_train, y_train, \\\n",
    "        batch_size=BATCH_SIZE, \\\n",
    "        epochs=EPOCHS, \\\n",
    "        verbose=1, \\\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[cp_callback])  # Pass callback to training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. TensorFlow Lite\n",
    "\n",
    "We use the TensorFlow Lite converter to convert the model into an optimized on for mobile devices. TensorFlow Lite uses the optimized FlatBuffer format to represent graphs. Therefore, a TensorFlow model needs to be converted into a FlatBuffer file before deploying it to Android. The TensorFlow Lite converter generates a TensorFlow Lite FlatBuffer file (.tflite) from a TensorFlow model. The converter script should be installed automatically on your PC if you use the latest versions of python and TensorFlow.\n",
    "\n",
    "For more information on the TensorFlow Lite and TensorFlow converter see: <a href=\"https://www.tensorflow.org/lite/convert/python_api\">Converter Python API guide</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18303908"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tensorflow.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"ws3_hwd_data/converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Android App\n",
    "\n",
    "NDK (Native Development Kit) is a tool that allows you to program in C/C++ for Android devices. It integrates with the SDK and is used for performance-critical code.\n",
    "\n",
    "* <a href=\"https://developer.android.com/ndk/downloads/\">Download and install Android NDK</a> (you should actually get an automatic invitation to install NDK from Android Studio once you try to build an app which requires its support).\n",
    "\n",
    "![alt text](https://jalammar.github.io/images/android-tensorflow-app-structure_2.png)\n",
    "\n",
    "* The HandWrittenDigits Android app is provided here: <a href=\"https://github.com/osaukh/mobile_computing_lab/tree/master/code/HandWrittenDigits\">source code</a>. It requires that you use TensorFlow verion > 1.9.1. Examine the code and run it on your hardware. You can also experiment with the model above and use your own model (copy your .tflite file to HandWrittenDigits/app/src/main/assets/).\n",
    "\n",
    "\n",
    "***\n",
    "## Related Examples and Useful Links\n",
    "\n",
    "* <a href=\"https://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow\">Not another MNIST tutorial with TensorFlow</a>\n",
    "* <a href=\"https://heartbeat.fritz.ai/intro-to-machine-learning-on-android-how-to-convert-a-custom-model-to-tensorflow-lite-e07d2d9d50e3\">Intro to Machine Learning on Androidâ€Šâ€”â€ŠHow to convert a custom model to TensorFlow Lite</a>\n",
    "* <a href=\"https://heartbeat.fritz.ai/introduction-to-machine-learning-on-android-part-2-building-an-app-to-recognize-handwritten-d58ebc01950\">Intro to Machine Learning on Android (Part 2): Building an app to recognize handwritten digits with TensorFlow Lite</a>\n",
    "\n",
    "\n",
    "***\n",
    "## Credits\n",
    "* YouTube video by Siraj Raval: <a href=\"https://www.youtube.com/watch?v=kFWKdLOxykE&feature=youtu.be\">A Guide to Running Tensorflow Models on Android</a>\n",
    "* <a href=\"https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android\">Original app source code for older versions of TensorFlow (<= 1.5.0)</a> adjusted by Siraj Raval\n",
    "* Original <a href=\"https://github.com/miyosuda/TensorFlowAndroidMNIST\">app source code</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
